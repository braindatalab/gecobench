{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7df40d",
   "metadata": {
    "id": "9f7df40d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "SEED=1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd4fdc8",
   "metadata": {
    "id": "1fd4fdc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjall/anaconda3/lib/python3.11/site-packages/openpyxl/worksheet/_read_only.py:79: UserWarning: Unknown extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "#open the excel file\n",
    "df_original=pd.read_excel('all_phrases.xlsx',index_col=None, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194cd17f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "194cd17f",
    "outputId": "1f3a4c90-1305-4a6c-a4c6-c50f5b9d5b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1687 (1610, 29) (1610, 29) (1610, 29) (1610, 29) (1610, 29)\n"
     ]
    }
   ],
   "source": [
    "df=df_original.copy()\n",
    "df=df[df[0].isna()] #here I deleted all the phrases that should be checked\n",
    "\n",
    "original_phrases=df[df[2]=='Original'].drop([0,1,2],axis=1).reset_index().drop(['index'],axis=1)\n",
    "\n",
    "#ground_truths\n",
    "all_female_phrases_gt=df[df[2]=='All Female'].drop([0,1,2],axis=1).reset_index().drop(['index'],axis=1)\n",
    "all_male_phrases_gt=df[df[2]=='All Male'].drop([0,1,2],axis=1).reset_index().drop(['index'],axis=1)\n",
    "female_sub_phrases_gt=df[df[2]=='Subject Female'].drop([0,1,2],axis=1).reset_index().drop(['index'],axis=1)\n",
    "male_sub_phrases_gt=df[df[2]=='Subject Male'].drop([0,1,2],axis=1).reset_index().drop(['index'],axis=1)\n",
    "\n",
    "print(int(df_original.shape[0]/5),original_phrases.shape,all_female_phrases_gt.shape,all_male_phrases_gt.shape,female_sub_phrases_gt.shape,male_sub_phrases_gt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4nBtYB93wOiC",
   "metadata": {
    "id": "4nBtYB93wOiC"
   },
   "source": [
    "# change name_female to name_female_subj/_obj "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "xKwWgN1CFA1Y",
   "metadata": {
    "id": "xKwWgN1CFA1Y"
   },
   "outputs": [],
   "source": [
    "original_phrases=df[df[2]=='Original'].drop([0,1,2],axis=1).reset_index().drop(['index'],axis=1)\n",
    "\n",
    "#ground_truths\n",
    "all_female_phrases_gt=df[df[2]=='All Female'].drop([0,1,2],axis=1).reset_index().drop(['index'],axis=1)\n",
    "all_male_phrases_gt=df[df[2]=='All Male'].drop([0,1,2],axis=1).reset_index().drop(['index'],axis=1)\n",
    "female_sub_phrases_gt=df[df[2]=='Subject Female'].drop([0,1,2],axis=1).reset_index().drop(['index'],axis=1)\n",
    "male_sub_phrases_gt=df[df[2]=='Subject Male'].drop([0,1,2],axis=1).reset_index().drop(['index'],axis=1)\n",
    "\n",
    "\n",
    "# with open('original_before.pkl', 'wb') as f:\n",
    "#     pickle.dump(original_phrases, f)\n",
    "    \n",
    "# with open('all_fem_before.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_female_phrases_gt, f) \n",
    "    \n",
    "# with open('all_male_before.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_male_phrases_gt, f)\n",
    "    \n",
    "# with open('subj_fem_before.pkl', 'wb') as f:\n",
    "#     pickle.dump(female_sub_phrases_gt, f)\n",
    "\n",
    "# with open('subj_male_before.pkl', 'wb') as f:\n",
    "#     pickle.dump(male_sub_phrases_gt, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96c59154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>she</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>surname_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>name_female_1</td>\n",
       "      <td>surname_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>surname_1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>name_female_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>surname_1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>her</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>She</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clergywoman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>name_female_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>she</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>name_female_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>name_female_2</td>\n",
       "      <td>name_female_3</td>\n",
       "      <td>surname_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>name_female_1</td>\n",
       "      <td>surname_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>name_female_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mother</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>NaN</td>\n",
       "      <td>sister</td>\n",
       "      <td>name_female_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>Her</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>name_female_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1610 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 3       4              5              6              7   \\\n",
       "0               NaN     NaN            NaN            she            NaN   \n",
       "1               She     NaN            NaN            NaN  name_female_1   \n",
       "2               She     NaN            NaN            NaN            NaN   \n",
       "3               She     NaN            NaN            NaN            NaN   \n",
       "4               NaN     NaN            NaN  name_female_1            NaN   \n",
       "...             ...     ...            ...            ...            ...   \n",
       "1605  name_female_1     NaN            NaN            NaN  name_female_2   \n",
       "1606            NaN     NaN            NaN  name_female_1      surname_1   \n",
       "1607  name_female_1     NaN         mother            NaN            NaN   \n",
       "1608            NaN  sister  name_female_1            NaN            NaN   \n",
       "1609            Her     NaN            NaN            NaN  name_female_1   \n",
       "\n",
       "                 8            9     10         11         12  ...   22   23  \\\n",
       "0               NaN          NaN  Mrs.  surname_1        NaN  ...  NaN  NaN   \n",
       "1         surname_1          NaN   NaN        NaN  surname_1  ...  NaN  NaN   \n",
       "2     name_female_1          NaN   NaN        NaN  surname_1  ...  NaN  NaN   \n",
       "3               NaN  clergywoman   NaN        NaN        NaN  ...  NaN  NaN   \n",
       "4               NaN          NaN   NaN        NaN        she  ...  NaN  NaN   \n",
       "...             ...          ...   ...        ...        ...  ...  ...  ...   \n",
       "1605  name_female_3    surname_1   NaN        NaN        NaN  ...  NaN  NaN   \n",
       "1606            NaN          NaN   NaN        NaN        NaN  ...  NaN  NaN   \n",
       "1607            NaN          NaN   NaN        NaN        NaN  ...  NaN  NaN   \n",
       "1608            NaN          NaN   NaN        NaN        NaN  ...  NaN  NaN   \n",
       "1609            NaN          NaN   NaN        NaN        NaN  ...  NaN  NaN   \n",
       "\n",
       "       24   25   26   27   28   29   30   31  \n",
       "0     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2     NaN  her  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4     NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "1605  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1606  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1607  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1608  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1609  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[1610 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_phrases\n",
    "all_female_phrases_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r0ZXW2jJwVBp",
   "metadata": {
    "id": "r0ZXW2jJwVBp"
   },
   "source": [
    "# ===================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa1e870",
   "metadata": {
    "id": "7fa1e870"
   },
   "outputs": [],
   "source": [
    "def change_words(original_phrase,ground_truth):\n",
    "    new_phrase=original_phrase.copy()\n",
    "    for j in list(ground_truth.index):\n",
    "        if not pd.isna(ground_truth.loc[j]):\n",
    "            new_phrase.loc[j]=ground_truth.loc[j]\n",
    "    return list(new_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85372cea",
   "metadata": {
    "id": "85372cea"
   },
   "outputs": [],
   "source": [
    "all_fem=[]\n",
    "all_male=[]\n",
    "subj_fem=[]\n",
    "subj_male=[]\n",
    "for idx in list(original_phrases.index):\n",
    "    all_fem_phrase=change_words(original_phrases.loc[idx],all_female_phrases_gt.loc[idx])\n",
    "    all_fem.append(all_fem_phrase)\n",
    "    \n",
    "    all_male_phrase=change_words(original_phrases.loc[idx],all_male_phrases_gt.loc[idx])\n",
    "    all_male.append(all_male_phrase)\n",
    "    \n",
    "    subj_fem_phrase=change_words(original_phrases.loc[idx],female_sub_phrases_gt.loc[idx])\n",
    "    subj_fem.append(subj_fem_phrase)\n",
    "    \n",
    "    subj_male_phrase=change_words(original_phrases.loc[idx],male_sub_phrases_gt.loc[idx])\n",
    "    subj_male.append(subj_male_phrase)\n",
    "    \n",
    "all_fem=pd.DataFrame(all_fem)\n",
    "all_male=pd.DataFrame(all_male)\n",
    "subj_fem=pd.DataFrame(subj_fem)\n",
    "subj_male=pd.DataFrame(subj_male)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7fbb53",
   "metadata": {
    "id": "ac7fbb53"
   },
   "source": [
    "## Changes to the names and surnames\n",
    "+ for the surnames I deleted the ones that had \"'\" such as O'brain (that's why there are only 248 surnames and not 250)\n",
    "+ for both I deleted the (sur)names that ended in \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4b40206",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4b40206",
    "outputId": "0dfe9b46-28ef-4475-b252-d75e577682b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 1) (250, 1) (248, 1)\n"
     ]
    }
   ],
   "source": [
    "female_names = pd.read_csv('Top250Female1996-2019.txt', lineterminator=\"\\n\", header=None)\n",
    "male_names = pd.read_csv('Top250Male1996-2019.txt', lineterminator=\"\\n\", header=None)\n",
    "surnames = pd.read_csv('Top250Surnames1991-2000.txt', lineterminator=\"\\n\", header=None) \n",
    "\n",
    "print(female_names.shape,male_names.shape,surnames.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bfddab1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bfddab1",
    "outputId": "bdbe1cf4-fb0b-405d-aa04-aeeddef2e33e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245 231 193\n"
     ]
    }
   ],
   "source": [
    "female_names=pd.DataFrame([name for name in list(female_names[0]) if name[-1]!='s'])\n",
    "male_names=pd.DataFrame([name for name in list(male_names[0]) if name[-1]!='s'])\n",
    "surnames=pd.DataFrame([name for name in list(surnames[0]) if (name[-1]!='s' or name.find(\"'\") != -1)])\n",
    "\n",
    "female_names=list(name[0] for name in female_names.values)\n",
    "male_names=list(name[0] for name in male_names.values)\n",
    "surnames=list(name[0] for name in surnames.values)\n",
    "\n",
    "print(len(female_names),len(male_names),len(surnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76960d18",
   "metadata": {
    "id": "76960d18"
   },
   "outputs": [],
   "source": [
    "\n",
    "def change_names(phrase):\n",
    "    p=phrase.copy()\n",
    "\n",
    "    for word_idx in list(p.index):\n",
    "        if str(p.loc[word_idx])[:9]=='name_male':\n",
    "            p.loc[word_idx]=male_names[int(p.loc[word_idx][-1])-1] #in these phrases the biggest amount of different names is 4 but if there are more than 9, use the str.rfind\n",
    "            #print(1)\n",
    "        elif str(p.loc[word_idx])[:11]=='name_female':\n",
    "            p.loc[word_idx]=female_names[int(p.loc[word_idx][-1])-1]\n",
    "            #print(2)\n",
    "        elif str(p.loc[word_idx])[:7]=='surname':\n",
    "            if p.loc[word_idx][-2:]=='pl':\n",
    "                p.loc[word_idx]=surnames[int(p.loc[word_idx][-4])-1]+'s'\n",
    "             #   print(3)\n",
    "            else:\n",
    "                p.loc[word_idx]=surnames[int(p.loc[word_idx][-1])-1]\n",
    "              #  print(4)\n",
    "    #print(p)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3f675fa",
   "metadata": {
    "id": "c3f675fa"
   },
   "outputs": [],
   "source": [
    "final_all_fem=[]\n",
    "final_all_male=[]\n",
    "final_subj_fem=[]\n",
    "final_subj_male=[]\n",
    "for idx in all_fem.index:\n",
    "    random.shuffle(female_names)\n",
    "    random.shuffle(male_names)\n",
    "    random.shuffle(surnames)\n",
    "    \n",
    "    phrase1=change_names(all_fem.iloc[idx])\n",
    "    final_all_fem.append(phrase1)\n",
    "    \n",
    "    phrase2=change_names(all_male.iloc[idx])\n",
    "    final_all_male.append(phrase2)\n",
    "    \n",
    "    phrase3=change_names(subj_fem.iloc[idx])\n",
    "    final_subj_fem.append(phrase3)\n",
    "    \n",
    "    phrase4=change_names(subj_male.iloc[idx])\n",
    "    final_subj_male.append(phrase4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c2d35ca",
   "metadata": {
    "id": "3c2d35ca"
   },
   "outputs": [],
   "source": [
    "final_all_fem=pd.DataFrame(final_all_fem)\n",
    "final_all_male=pd.DataFrame(final_all_male)\n",
    "final_subj_fem=pd.DataFrame(final_subj_fem)\n",
    "final_subj_male=pd.DataFrame(final_subj_male)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4efa9f5c",
   "metadata": {
    "id": "4efa9f5c"
   },
   "outputs": [],
   "source": [
    "def create_train_val_datasets(final_male,final_female):\n",
    "    SEED=1234\n",
    "    random.seed(SEED)\n",
    "    frac=0.8\n",
    "    indexes=list(final_male.index)\n",
    "    train_idx=random.sample(indexes, k = int(len(indexes)*frac))\n",
    "    val_idx=[i for i in list(range(len(final_male))) if i not in train_idx]\n",
    "    \n",
    "    #trianing dataset\n",
    "    #selects the lines for the training dataset\n",
    "    #these are two datasets that will then be used to choose between female and male after\n",
    "    df_training_male=final_male.iloc[train_idx]\n",
    "    df_training_female=final_female.iloc[train_idx]\n",
    "\n",
    "    #creating training dataset\n",
    "    indexes_male=random.sample(list(df_training_male.index), k = int(len(list(df_training_male.index))/2))\n",
    "    indexes_female=[i for i in train_idx if i not in indexes_male]\n",
    "    \n",
    "    \n",
    "    #selects the lines that are female or male from the training datasets\n",
    "    df_male=df_training_male.loc[indexes_male]\n",
    "    df_female=df_training_female.loc[indexes_female]\n",
    "\n",
    "    df_male.insert(0, 'target', 1)\n",
    "    df_female.insert(0, 'target', 0)\n",
    "    \n",
    "    #concatenate dataframes to create dataset\n",
    "    training_df= pd.concat([df_male,df_female]).sort_index()\n",
    "   \n",
    "    #validation dataset    \n",
    "    #selects the lines for the validation dataset\n",
    "    df_validation_male=final_male.iloc[val_idx]\n",
    "    df_validation_female=final_female.iloc[val_idx]\n",
    "    \n",
    "    df_validation_male.insert(0, 'target', np.ones(len(df_validation_male)))\n",
    "    df_validation_female.insert(0, 'target', np.zeros(len(df_validation_female)))\n",
    "    \n",
    "    return training_df,df_validation_male,df_validation_female,val_idx,train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c7d55d",
   "metadata": {
    "id": "a4c7d55d"
   },
   "outputs": [],
   "source": [
    "training_df_all,df_validation_male_all,df_validation_female_all,val_idx_all,train_idx_all=create_train_val_datasets(final_all_male,final_all_fem)\n",
    "training_df_subj,df_validation_male_subj,df_validation_female_subj,val_idx_subj,train_idx_subj=create_train_val_datasets(final_subj_male,final_subj_fem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e875f38",
   "metadata": {
    "id": "9e875f38"
   },
   "outputs": [],
   "source": [
    "with open('training_df_all.pkl', 'wb') as f:\n",
    "    pickle.dump(training_df_all, f)\n",
    "    \n",
    "with open('df_validation_male_all.pkl', 'wb') as f:\n",
    "    pickle.dump(df_validation_male_all, f)\n",
    "\n",
    "with open('df_validation_female_all.pkl', 'wb') as f:\n",
    "    pickle.dump(df_validation_female_all, f)\n",
    "\n",
    "with open('training_df_subj.pkl', 'wb') as f:\n",
    "    pickle.dump(training_df_subj, f)\n",
    "    \n",
    "with open('df_validation_male_subj.pkl', 'wb') as f:\n",
    "    pickle.dump(df_validation_male_subj, f)\n",
    "\n",
    "with open('df_validation_female_subj.pkl', 'wb') as f:\n",
    "    pickle.dump(df_validation_female_subj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "P9X1mU429ENA",
   "metadata": {
    "id": "P9X1mU429ENA"
   },
   "outputs": [],
   "source": [
    "len(original_phrases)\n",
    "#creation of the ground truth\n",
    "#all dataset\n",
    "gt_subj = pd.DataFrame(np.zeros(female_sub_phrases_gt.shape))\n",
    "gt_all = pd.DataFrame(np.zeros(female_sub_phrases_gt.shape))\n",
    "\n",
    "for i in range(len(original_phrases)):\n",
    "  phrase_f=all_female_phrases_gt.iloc[i]\n",
    "  phrase_m=all_male_phrases_gt.iloc[i]\n",
    "  for word_idx in range(len(phrase_f)):\n",
    "     word_idx+=3\n",
    "     if str(phrase_f[word_idx])!=str(phrase_m[word_idx]):\n",
    "       gt_all.iloc[i][word_idx]=1\n",
    "\n",
    "\n",
    "#subj dataset\n",
    "gt_subj = pd.DataFrame(np.zeros(female_sub_phrases_gt.shape))\n",
    "gt_subj\n",
    "\n",
    "for i in range(len(original_phrases)):\n",
    "  phrase_f=female_sub_phrases_gt.iloc[i]\n",
    "  phrase_m=male_sub_phrases_gt.iloc[i]\n",
    "  for word_idx in range(len(phrase_f)):\n",
    "     word_idx+=3\n",
    "     if str(phrase_f[word_idx])!=str(phrase_m[word_idx]):\n",
    "       gt_subj.iloc[i][word_idx]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "oN9QuXecBHO1",
   "metadata": {
    "id": "oN9QuXecBHO1"
   },
   "outputs": [],
   "source": [
    "gt_subj_val=gt_subj.copy().loc[val_idx_subj]\n",
    "gt_subj_train=gt_subj.copy().loc[train_idx_subj]\n",
    "gt_all_val=gt_all.copy().loc[val_idx_subj] # Should this be val_idx_all?\n",
    "\n",
    "# Should this be train_idx_all? \n",
    "# Maybe not severe since we use the same seed\n",
    "gt_all_train=gt_all.copy().loc[train_idx_subj] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afAqTBtDFtKc",
   "metadata": {
    "id": "afAqTBtDFtKc"
   },
   "outputs": [],
   "source": [
    "with open('gt_subj_val.pkl', 'wb') as f:\n",
    "    pickle.dump(gt_subj_val, f)\n",
    "    \n",
    "with open('gt_subj_train.pkl', 'wb') as f:\n",
    "    pickle.dump(gt_subj_train, f)\n",
    "    \n",
    "with open('gt_all_val.pkl', 'wb') as f:\n",
    "    pickle.dump(gt_all_val, f)\n",
    "    \n",
    "with open('gt_all_train.pkl', 'wb') as f:\n",
    "    pickle.dump(gt_all_train, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data_all_same and data_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the two datasets\n",
    "+ There are two datasets:\n",
    "    + one where the sentences are completly female or completly male\n",
    "    + one where the subject (and words related to it) are female or male\n",
    "\n",
    "+ For each dataset randomly half of the indexes are chosen to be male and the other half female and these are the phrases that will have a target value of 0 for female and 1 for male.\n",
    "\n",
    "### All same gender\n",
    "+ from the datasets:\n",
    "    + final_all_male\n",
    "    + final_all_female\n",
    "+ to the dataset:\n",
    "    + data_all_same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Creating datasets:\n",
    "    + receives a dataframe of phrases/tokens\n",
    "    + chooses the indexes that will be male\n",
    "    + adds a column in the beginning with the target value\n",
    "    + saves the dataset\n",
    "    + returns dataset and male indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac26a4c6",
   "metadata": {
    "id": "ac26a4c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         Shortly\n",
      "1      afterwards\n",
      "2               ,\n",
      "3              he\n",
      "4          visits\n",
      "5     Netherfield\n",
      "6               ,\n",
      "7             Mr.\n",
      "8           Patel\n",
      "9              's\n",
      "10         rented\n",
      "11      residence\n",
      "12              ,\n",
      "13           much\n",
      "14             to\n",
      "15            Mr.\n",
      "16           Bird\n",
      "17             's\n",
      "18        delight\n",
      "19              .\n",
      "20            NaN\n",
      "21            NaN\n",
      "22            NaN\n",
      "23            NaN\n",
      "24            NaN\n",
      "25            NaN\n",
      "26            NaN\n",
      "27            NaN\n",
      "28            NaN\n",
      "Name: 0, dtype: object\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#randomly choosing the indexes that will be male (target=1)\n",
    "indexes=list(range(len(final_subj_male)))\n",
    "indexes_male=random.sample(indexes, k = int(len(indexes)/2))\n",
    "\n",
    "#creating the target\n",
    "target=np.zeros((len(final_subj_male)))\n",
    "target[indexes_male]=1\n",
    "\n",
    "#choosing the phrases\n",
    "data_all_same=[]\n",
    "for i in range(len(final_subj_male)):\n",
    "    if i in indexes_male:\n",
    "        data_all_same.append(final_all_male.loc[i])\n",
    "    else:\n",
    "        data_all_same.append(final_all_fem.loc[i])\n",
    "data_all_same=pd.DataFrame(data_all_same)\n",
    "data_all_same\n",
    "\n",
    "#saving the data_set+target\n",
    "data = [[data_all_same.loc[i],target[i]] for i in range(len(target))]\n",
    "\n",
    "print(data[0][0])\n",
    "print(data[0][1])\n",
    "\n",
    "# ??? First the file gets written by Create_Dataset and then the data_all_same.pkl is overwritten\n",
    "with open('data_all_same.pkl', 'wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only the subject is the same gender\n",
    "+ from the datasets:\n",
    "    + final_subj_male\n",
    "    + final_subj_fem\n",
    "+ to the dataset:\n",
    "    + data_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly choosing the indexes that will be male (target=1)\n",
    "indexes=list(range(len(final_subj_male)))\n",
    "indexes_male=random.sample(indexes, k = int(len(indexes)/2))\n",
    "\n",
    "#creating the target\n",
    "target=np.zeros((len(final_subj_male)))\n",
    "target[indexes_male]=1\n",
    "\n",
    "#choosing the phrases\n",
    "data_subj=[]\n",
    "for i in range(len(final_subj_male)):\n",
    "    if i in indexes_male:\n",
    "        data_subj.append(final_subj_male.loc[i])\n",
    "    else:\n",
    "        data_subj.append(final_subj_fem.loc[i])\n",
    "        \n",
    "data_subj=pd.DataFrame(data_subj)\n",
    "\n",
    "#saving the data_set+target\n",
    "data=[[data_subj.loc[i],target[i]] for i in range(len(target))]\n",
    "\n",
    "with open('data_subj.pkl', 'wb') as f:\n",
    "    pickle.dump(data,f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
