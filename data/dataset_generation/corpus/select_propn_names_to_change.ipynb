{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "DATA_PATH = \"../data/corpus_data\"\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'nouns_keep.pkl'), 'rb') as f:\n",
    "    nouns_keep = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'PROPN_keep.pkl'), 'rb') as f:\n",
    "    PROPN_keep=pickle.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'pronouns_keep.pkl'), 'rb') as f:\n",
    "    pronouns_keep=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_propn(doc):\n",
    "    #doc is a spacy doc sentence \n",
    "    propns=[]\n",
    "    propns_text=[]\n",
    "    for token in doc:\n",
    "        if token.pos_=='PROPN':\n",
    "            propns.append(token)\n",
    "            propns_text.append(token.text)\n",
    "    return [propns,propns_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding all the proper nouns of the three sentence lists to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pn=[]\n",
    "for i in nouns_keep:\n",
    "    pn.append(get_propn(i))\n",
    "\n",
    "for i in PROPN_keep:\n",
    "    pn.append(get_propn(i))\n",
    "\n",
    "for i in pronouns_keep:\n",
    "    pn.append(get_propn(i))\n",
    "\n",
    "pn[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting those that are repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pn_str=[name.text for name in pn]\n",
    "pn_str=[name[1] for name in pn]\n",
    "\n",
    "# Flatten list of lists\n",
    "pn_str = [item for sublist in pn_str for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(pn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pn_str_dif=list(set(pn_str)) #names that are not repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pn_str_dif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goddess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "names= [\"Alice\",\"Eliza\",\"Nora\",\"Hester\",\"Edna\",\"Mercedes\",\"Lilias\",\"Rachael\",\"Mary\",\"Martha\",\"Emily\",\"Lydia\",\"Sophia\",\"Kamala\",\"Diana\",\"Georgiana\",\"Lizaveta\",\"Jennie\",\"Eliza\",\"George\",\"Chopin\",\"Seryozha\",\"Arete\",\"Micomicona\",\"Weena\",\"Sofya\",\"Jerry\",\"Sibyl\",\"Rudy\",\"Hélène\",\"Hera\",\"Mercédès\",\"Helen\",\"Isagani\",\n",
    "\"Alice\",\"Quiroga\",\"Tiani\",\"Fanny\",\"Agnes\",\"Louisa\",\"Harthouse\",\"Sissy\",\"Emma\",\"Harriet\",\"Stiva\",\"Ona\",\"Marija\",\"Isabella\",\"Cathy\",\"Irene\",\"Eliza\",\"Eva\",\"Topsy\",\"Kapitán\",\"María\",\"Clara\",\"Nancy\",\"Henry\",\"Eliza\",\"Alfred\",\"Thornton\",\"Rose\",\"Cornelia\",\"Lucy\",\"Katrina\",\"Ichabod\",\"Marianne\",\"Hope\",\"Ophelia\",\n",
    "\"Emmeline\",\"Cassy\",\"Eva\",\"Charlotte\",\"Annie\",\"Edgar\",\"Catherine\",\"Karenina\",\"Arkadyevna\",\"Anna\",\"Charlotte\",\"Penelope\",\"Clytemnestra\",\"Lily\",\"Valentine\",\"Myriel\",\"Grushenka\",\"Cosette\",\"Éponine\",\"Marilla\",\"Athena\",\"Calypso\",\"Anne\",\"Alyosha\",\"Katerina\",\"Beth\",\"Lizzie\",\"Fan\",\"Em\",\"Marmee\",\"Laurie\",\"Beth\",\n",
    "\"Meg\",\"Jo\",\"Amy\",\"Dorothy\",\"Jane\",\"Maimie\",\"Eugénie\",\"Lucie\",\"Kristine\",\"Mina\",\"Molly\",\"Dunya\",\"Gwendolen\",\"Biddy\",\"Hester\",\"Sonya\",\"Katerina\",\"Rachel\",\"Lucy\",\"Myrtle\",\"Daisy\",\"Chriseis\",\"Briseis\",\"Éponine\",\"Eurycleia\",\"Louisa\",\"Clara\",\"Elizabeth\",\"Marie\",\"Riborg\",\"Louisa\",\"Sissy\",\"Penelope\",\"Fanny\",\n",
    "\"Elinor\",\"Azelma\",\"Fantine\",\"Cosette\",\"Anna\",\"Mikhaylovna\",\"Natalya\",\"Natasha\",\"Thetis\",\"Anna\",\"Athena\",\"Eva\",\"Adèle\",\"Kitty\",\"Edna\",\"Frank\",\"Pearl\",\"Grete\",\"Sally\",\"Blake\",\"Milly\",\"Gerty\",\"Mina\",\"Maria\",\"Petya\",\"Chryseis\",\"Briseis\",\"Athena\",\"Wendy\",\"Hélène\",\"Natasha\",\"Sonya\",\"Hecuba\",\"Bella\",\"Lucy\",\n",
    "\"Estella\",\"Grace\",\"Jane\",\"Sarah\",\"Blanche\",\"Henry\",\"Nestor\",\"Hector\",\"Fagin\",\"Gabriel\",\"James\",\"Jefferson\",\"Pedro\",\"Ben\",\"Tommy\",\"Siddhartha\",\"William\",\"Scharff\",\"Giroflée\",\"Scully\",\"Connor\",\"Leibniz\",\"Abbé\",\"Pete\",\"Thornton\",\"Hal\",\"Buck\",\"Charles\",\"Oliver\",\"Hans\",\"Alcée\",\"Colin\",\"Charles\",\"Friedrich\",\n",
    "\"Gervais\",\"Enjolras\",\"Léonce\",\"Alexei\",\"Anastacio\",\"Kirillovich\",\"Gavroche\",\"Eumaeus\",\"Odysseus\",\"Phemius\",\"Alcinous\",\"Fyodor\",\"Ivan\",\"Fauchelevent\",\"Israel\",\"Flint\",\"Ben\",\"Cratchit\",\"Tin\",\"Florentino\",\"Semyon\",\"Roger\",\"Dimmesdale\",\"Jim\",\"Zakharovich\",\"Bantam\",\"Hester\",\"Alec\",\"Patroclus\",\"Petya\",\"Walden\",\n",
    "\"Pyotr\",\"Throwaway\",\"Garrett\",\"John\",\"Capitán\",\"Basil\",\"Herbert\",\"Renfield\",\"Dracula\",\"Dorian\",\"David\",\"James\",\"Josiah\",\"Bitzer\",\"Junior\",\"Thomas\",\"Jurgis\",\"Connor\",\"Jack\",\"Antinous\",\"Hindley\",\"Hareton\",\"Thrushcross\",\"George\",\"Crisóstomo\",\"Bernardo\",\"Elías\",\"Fagin\",\"Freddy\",\"Pickering\",\"Buck\",\"Spitz\",\n",
    "\"Faustus\",\"Edward\",\"Brandon\",\"Drebber\",\"Cornelius\",\"Lucifer\",\"Beelzebub\",\"Mephistophilis\",\"Nathaniel\",\"Grinnel\",\"Oliver\",\"Pedro\",\"Jacobs\",\"Basilio\",\"Pedro\",\"Padre\",\"Arthur\",\"Harry\",\"Heathcliff\",\"Robert\",\"MaJavert\",\"Fyodor\",\"Zosima\",\"Telemachus\",\"Odysseus\",\"Demodocus\",\"Don\",\"Dulcinea\",\"Nestor\",\"Rocinante\",\n",
    "\"Andrés\",\"Mentes\",\"Gilbert\",\"Quixano\",\"Alyosha\",\"Kolya\",\"Ilyusha\",\"Maximilien\",\"Fred\",\"Ivan\",\"Alyosha\",\"Dmitri\",\"Fyodor\",\"Marius\",\"Becky\",\"Huckleberry\",\"Jim\",\"Muff\",\"Injun\",\"John\",\"Alcott\",\"Henry\",\"Boq\",\"Andrea\",\"Albert\",\"Danglars\",\"Caderousse\",\"Bertuccio\",\"Richard\",\"Tootles\",\"Nibs\",\"Edmond\",\"Slightly\",\n",
    "\"Curly\",\"Poole\",\"Tinker\",\"Joe\",\"Orlick\",\"Herbert\",\"Huck\",\"Jack\",\"Algernon\",\"Ernest\",\"Clarriker\",\"Marlow\",\"Romanovich\",\"Porfiry\",\"Rodion\",\"Gregor\",\"Startop\",\"Pip\",\"Herbert\",\"Bentley\",\"John\",\"Hastie\",\"Oliver\",\"Jordan\",\"Carton\",\"Samuel\",\"Daggoo\",\"James\",\"Dorian\",\"Bildad\",\"Queequeg\",\"Peleg\",\"Ishmael\",\"Charley\",\n",
    "\"Artful\",\"Nick\",\"Henry\",\"Brandon\",\"David\",\"Stephen\",\"Charles\",\"Edwin\",\"Bernard\",\"John\",\"Edward\",\"William\",\"Buck\",\"Polydamas\",\"Dantès\",\"Booker\",\"Pierre\",\"Pyotr\",\"Andrei\",\"Platon\",\"Achilles\",\"Homer\",\"Peter\",\"Odysseus\",\"Pandaros\",\"Menelaus\",\"Polydamas\",\"Hector\",\"Achilles\",\"Hermes\",\"Priam\",\"Patroclus\",\"Apollo\",\n",
    "\"Anatole\",\"Nikolai\",\"Napoleon\",\"Andrei\",\"Marius\",\"Frederick\",\"T.\",\"Ken\",\"Arkady\",\"Huck\",\"Jim\",\"Stephen\",\"Boris\",\"Andrei\",\"Anatole\",\"Apollo\",\"Achilles\",\"Patroclus\",\"Tom\",\"Simon\",\"Harry\",\"Elías\",\"Crisóstomo\",\"George\",\"Léonce\",\"Robert\",\"Ares\",\"Zeus\",\"Diomedes\",\"Agamemnon\",\"James\",\"Silas\",\"Ivanovich\",\"Torvald\",\n",
    "\"Tashtego\",\"Kurtz\",\"Gregor\",\"Ahab\",\"Marlow\",\"Fedallah\",\"Abraham\",\"Danvers\",\"Pip\",\"Joe\"]\n",
    "\n",
    "surname=['Gatsby','Gatz','Twist' ,'Raskolnikov','Williams','Sowerby','Weatherstaff' ,'Vasudeva' ,'Walter' ,'Elliot' ,'Croft' ,'Steerforth','Kamaswami' ,'Traddles' ,\n",
    "'Candide' ,'Copperfield'  ,'Steerforth'   ,'Craven' ,'Medlock' ,'Drebber' ,'Stangerson' ,'Hope' ,'Faria','Busoni','Russell','Benwick','Darcy','Woodman','Hands'  ,'Louis','Taylor' ,\n",
    "'Weston' ,'Vronsky','Tashkent','Harris'     ,'Gunn' ,'Bhaer','Pavlovich'  ,'Dimitri' ,'Ivanovna' ,'Wickham' ,'Chillingworth','Marmeladov' ,'Watson' ,'Lazarus','Deasy','Clifford',\n",
    "'Stanhope','Versailles'  ,'Villefort' ,'Priam','Darling'  ,'Thérien' ,'Pond'  ,'Lyons','Ilyich'   ,'Raskolnikov'   ,'Vane' ,'Gray' ,'Burns' ,'Pocket' ,'Hallward','Midwesterner' ,\n",
    "'Marquis','Pross'   ,'Wickham' ,'Simoun' ,'Basilio' ,'Leeds'  ,'Salví' ,'Dickon','Wentworth'  ,'Musgrove','Hayter','Smith' ,'Benwick'  ,'Wentworth'  ,'Gummidge','Micawbers'  ,\n",
    "'Peggotty' ,'Wickfield' ,'Bounderby','Creakle'  ,'Pooh' ,'Gradgrind' ,'Sleary' ,'Duane','Sparsit','Harthouse'   ,'Blackpool' ,'Sparsit' ,'Gradgrind' ,'Vronsky','Lockwood',\n",
    "'Grange' ,'Wiltshire' ,'Shelby','Tiago' ,'Salví' ,'Dámaso' ,'Gulliver','Lilliputians','Blefuscudians','Glubbdubdrib','Valdes','Holmes','Charpentier' , 'Crane' ,'Steele' ,\n",
    "'Willoughby'  ,'Wagner' ,'Parker' ,'Willis' ,'Balnibarbi' ,'Bruce' ,'Houyhnhnms'    ,'Brownlow','Bedwin' ,'Doolittle' ,'Pearce' ,'Maylie' ,'Higgins'  ,'Sowerberrys' ,\n",
    "'Eibarramendia'   ,'Loker' ,'Legree'   ,'Martin','Knightley'  ,'Elton' ,'Westons' ,'Ratignolle' ,'Reisz' ,'Lebrun' ,'Bhaer','Hummels','Pavlovich','Quixote','Sancho', 'Cardenio'  ,\n",
    "'Smerdyakov' ,'Samsonov' ,'Karamazov'  ,'Ivanovna' ,'Potter','Finn','Gillenormand'  , 'Javert'  ,'Thatcher' ,'Joe' ,'Cavalcanti','Laurence' , 'Villefort' ,'Brooke'  ,'Barsad',\n",
    "'Defarge','Lanyon','Pan','Smee' ,'Dantès' , 'Hook' ,'Mannering' ,'Bell' ,'Richmond','Magwitch','Dimmesdale','Samsa','Twain' ,'Bracknell' ,'Raskolnikov','Luzhin' ,'Zamyotov',\n",
    "'Ivanovna'  , 'Petrovich','Razumikhin'   ,'Marmeladov'  ,'Raskolnikov'  ,'Wemmick','Varens' ,'Eyre'  ,'Brocklehurst' ,'Gargery' ,'Compeyson'  ,'Jaggers' ,'Havisham' ,\n",
    "'Enfield','Drummle' ,'Rank','Enderby','Harker' ,'Quincey' ,'Holmwood' ,'Dracula'  ,'Starbuck'  ,'Krogstad'  ,'Collins','Darcy','Baker','Darnay'  ,'Pross','Lorry','Manette' ,\n",
    "'Bloom'  ,'Purefoy' ,'Rostov' ,'Dolokhov','Rostovs' ,'Karataev'    ,'Buchanan','Kirilovich' , 'Bezukhov' ,'Lucas' ,'Gardiner'  ,'Phelps','Bates','Dodger','Higgins','Jekyll',\n",
    " 'Brownlow','Gradgrind','Bounderby','Peggotty','Andersen' ,'Bennet' ,'Bingley' ,'Musgroves' ,'Voigt' ,'Andersdatter'    ,'Murdstone'  ,'Watson' ,'Holmes','Shaw','Leeford',\n",
    " 'Ferrars','Gulliver' ,'Danglars','Valjean' , 'Thénardier' ,'Washington', 'Scrooge' ,'Morris'  ,'Douglass'  ,'Deasy', 'Rostovs' ,'Rostova' ,'Drubetskaya','Vronsky','Levin',\n",
    " 'Karenin' ,'Guevarra' , 'Tiburcio' ,'Legree'  ,'Reisz' ,'Churchill' ,'Svidrigailov' ,'Starbuck','Bellingham' ,'Chillingworth' ,'Seward','Reed','Rochester ', 'Utterson',\n",
    "  'Hyde' ,'Havisham','Jaggers' ,'Wemmick'   , 'Jekyll', 'Lanyon' , 'Carew'  ]\n",
    "\n",
    "complex_name=[ 'de Mendez' ,'Doña Victorina de los Reyes de de Espadaña', 'Oye - Eboe','van Helsing','Van Tassel','Patron - Minette',\n",
    "'Eynsford - Hills ', 'Tiny Tim' ] #these can be changes for just one name \"the wizard of oz\"\n",
    "\n",
    "names_as_propn=['King','Queen','Knave of Hearts','Red Queen','Gryphon' ,'Wicked Witch of the West ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def highlight_word(phrase,colour_propn):\n",
    "    # phrase is a doc string\n",
    "    # colour is a string with rgb values (ex. 'rgb(155,217,230)')\n",
    "\n",
    "    sent=[]\n",
    "\n",
    "    for token in phrase:\n",
    "        if token.text in pn_str_dif:\n",
    "            sent.append(\" <span style='background: {}'>{}</span> \".format(colour_propn,token.text))\n",
    "        else:\n",
    "            sent.append(token.text)\n",
    "\n",
    "    \n",
    "    return ' '.join(sent) #returns a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nouns_after\n",
    "# propn_after_single \n",
    "# phrases_pron\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "all_sentences=nouns_keep+PROPN_keep+pronouns_keep\n",
    "\n",
    "p=nouns_keep[1]\n",
    "\n",
    "display(HTML(highlight_word(p,'rgb(25, 108, 56)')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'all_sentences.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_propn(all_sentences[1])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "already_added_propn=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1500,len(all_sentences)):\n",
    "    prop=get_propn(all_sentences[i])[1]\n",
    "\n",
    "    if not all(item in already_added_propn for item in prop): #if all the proper names havent appear yet\n",
    "        print(i)\n",
    "        display(HTML(highlight_word(all_sentences[i],'rgb(188, 108, 37)')))\n",
    "    \n",
    "    already_added_propn+=prop\n",
    "    #already_added_propn=list(set(already_added_propn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(DATA_PATH, 'all_sentences.pkl'),'rb') as f:\n",
    "    phrases=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "workbook=xlsxwriter.Workbook(os.path.join(DATA_PATH, \"Phrases.xlsx\"))\n",
    "worksheet=workbook.add_worksheet('Phrases')\n",
    "\n",
    "cell_format_names=workbook.add_format()\n",
    "cell_format_names.set_bg_color('#95d5b2')\n",
    "\n",
    "cell_format_pron=workbook.add_format()\n",
    "cell_format_pron.set_bg_color('#ddb892')\n",
    "\n",
    "row_index=0\n",
    "\n",
    "for p_idx in range(len(phrases)):\n",
    "    worksheet.write(row_index,0, \"Original\")\n",
    "    for token_idx in range(0, len(phrases[p_idx])):\n",
    "        token = phrases[p_idx][token_idx]\n",
    "        worksheet.write(row_index,token_idx + 1, token.text)\n",
    "    \n",
    "    for key in [\"All Female\", \"All Male\", \"Subject Female\", \"Subject Male\"]:\n",
    "        row_index += 1\n",
    "        worksheet.write(row_index, 0, key)\n",
    "    \n",
    "    row_index += 1\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0568189e475ff095f8d58e708f11d5183d1019c3e0d991b7e7d0732f0bba373"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
