{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the gender corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: with the pickle files you just need to run from the points that have =====. This way you don't have to re-scrape the wikipedia, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this dataset is to create a ground truth to test NLP models. To do so, the dataset is composed of a set of manipulated phrases where every phrase has:\n",
    "+ the original phrase\n",
    "+ 2 phrases where only the subject is female and male\n",
    "+ 2 phrases where everything (both the subject and the object) are female and male\n",
    "\n",
    "The original phrases are obtained from Wikipedia entries from the books that appeared as the most popular in the day 17/3/2022 of the Gutemberg project. This list has been saved for later use (as well as the list with the author of the book)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Getting the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:44:26.892858550Z",
     "start_time": "2023-12-04T10:44:26.686333688Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "DATA_PATH = \"../data/corpus_data\"\n",
    "os.makedirs(DATA_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:44:40.668372221Z",
     "start_time": "2023-12-04T10:44:34.162363346Z"
    }
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "url_book_names='https://www.gutenberg.org/browse/scores/top'\n",
    "driver.get(url_book_names)\n",
    "\n",
    "soup=BeautifulSoup(driver.page_source,'lxml')\n",
    "tr_p1=soup.find_all('tr')\n",
    "#list 100 ebooks yesterday (17/3/2022)\n",
    "list_xpath='/html/body/div[1]/div/ol[1]'\n",
    "book_titles=driver.find_element(By.XPATH, list_xpath)\n",
    "book_list=book_titles.text.split('\\n')\n",
    "book_and_author=book_titles.text.split('\\n')\n",
    "\n",
    "for i in range(len(book_list)):\n",
    "    by=book_list[i].find(' by ')\n",
    "    book_list[i]=book_list[i][:by]\n",
    "    book_and_author[i]=book_and_author[i][:book_and_author[i].find(' (')]\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'book_list.pkl'), 'wb') as f:\n",
    "    pickle.dump(book_list, f)\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'book_and_author.pkl'), 'wb') as f:\n",
    "    pickle.dump(book_and_author, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get Wikipedia entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:45:46.352224785Z",
     "start_time": "2023-12-04T10:45:46.329595457Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(DATA_PATH, 'book_list.pkl'), 'rb') as f:\n",
    "    books = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'book_and_author.pkl'), 'rb') as f:\n",
    "    book_and_author = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:45:49.383482878Z",
     "start_time": "2023-12-04T10:45:49.366650537Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "url_books=[]\n",
    "for book in book_and_author:\n",
    "    plot=[]\n",
    "    url_google='https://www.google.com/search?q=\"en.wikipedia.org\"+'\n",
    "    for i in book:\n",
    "        if i in string.punctuation:\n",
    "            book=book.replace(i,'')\n",
    "        #book=book.replace(i,' ')\n",
    "    url_books.append(url_google+book.replace(' ','+'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:45:53.973685565Z",
     "start_time": "2023-12-04T10:45:53.955042513Z"
    }
   },
   "outputs": [],
   "source": [
    "print(url_books[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get to the Wikipedia of all of these books, firstly it is needed to get to google and accept the cookies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:46:30.177649349Z",
     "start_time": "2023-12-04T10:46:27.363274314Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "part_of_wiki=[]\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.google.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:46:33.505109034Z",
     "start_time": "2023-12-04T10:46:33.483223212Z"
    }
   },
   "outputs": [],
   "source": [
    "#this is in its own cell because sometimes the brower notices that we are a robot and we need to close it and re-run the previous cell\n",
    "n=0\n",
    "sentences=[] #list of all 3rd person sentences of all book wikipedia pages\n",
    "texts=[] # list of the texts of the entire wikipedia pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:47:15.197972477Z",
     "start_time": "2023-12-04T10:46:35.854258032Z"
    }
   },
   "outputs": [],
   "source": [
    "#main loop :\n",
    "#       gets in the wikipedia pages of the 100 chosen books\n",
    "#       scraps all text from the paragraphs in each page\n",
    "#       applies the get_3rd_person_phrases(text) to keep only the phrases in the 3rd person\n",
    "\n",
    "for url in url_books:\n",
    "    #getting to the wikipedia page, after knowing the url\n",
    "    n+=1\n",
    "    \n",
    "    driver.get(url)\n",
    "    driver.find_element(By.PARTIAL_LINK_TEXT, \"Wikipedia\").click()\n",
    "    \n",
    "    #getting the text on the page\n",
    "    wiki_paragraphs=driver.find_elements(By.TAG_NAME, 'p')\n",
    "    for i in wiki_paragraphs:\n",
    "        t=i.text\n",
    "        texts.append(t)\n",
    "    \n",
    "    print(n)\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wikipedia references in these paragraphs were deleted and the texts saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:47:27.577539335Z",
     "start_time": "2023-12-04T10:47:27.541752351Z"
    }
   },
   "outputs": [],
   "source": [
    "# removing the [1] wikipedia references\n",
    "import re\n",
    "for i in range(len(texts)):\n",
    "    texts[i]=re.sub( r'\\[.*?\\]', '', texts[i])\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'raw_texts.pkl'), 'wb') as f:\n",
    "    pickle.dump(texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. From Wikipedia paragraphs to SpaCy sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:47:30.934322366Z",
     "start_time": "2023-12-04T10:47:30.921676169Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(DATA_PATH, 'raw_texts.pkl'), 'rb') as f:\n",
    "    texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:47:33.255047780Z",
     "start_time": "2023-12-04T10:47:33.235276101Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:47:35.057917034Z",
     "start_time": "2023-12-04T10:47:35.038271723Z"
    }
   },
   "outputs": [],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:49:29.872426638Z",
     "start_time": "2023-12-04T10:49:21.722241773Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:49:41.436992074Z",
     "start_time": "2023-12-04T10:49:40.580922057Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:50:06.160975577Z",
     "start_time": "2023-12-04T10:49:52.241372093Z"
    }
   },
   "outputs": [],
   "source": [
    "texts_span=[]\n",
    "for paragraph in texts:\n",
    "    phrase_gen=nlp(paragraph).sents #spacy generator\n",
    "    for p in phrase_gen:\n",
    "        texts_span.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(texts_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in texts_doc each phrase is a spacy doc\n",
    "texts_docs=[nlp(phrase.text) for phrase in texts_span]\n",
    "#this cell takes quite a bit so I will pickle texts_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_docs[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(DATA_PATH, 'spacy_doc_from_raw_texts_wiki.pkl'), 'wb') as f:\n",
    "    pickle.dump(texts_docs,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Selecting phrases which have the ROOT verb in the 3rd person singular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'spacy_doc_from_raw_texts_wiki.pkl'), 'rb') as f:\n",
    "    texts_docs=pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roots_third_person(text):\n",
    "    #text is a list of spacy docs that correspond to sentences\n",
    "\n",
    "    third_person_phrases=[] #phrases in the 3rd person\n",
    "    roots=[] # verbs that the model considers to be the verb of the independent sentence (root verb)\n",
    "    root_childs=[] #subjects and objects of the root verb (?)\n",
    "\n",
    "    for phrase in text:\n",
    "        for token in phrase:\n",
    "            if token.dep_=='ROOT':\n",
    "                if token.morph.get('Person')==['3'] and token.morph.get('Number')==['Sing']:\n",
    "                    third_person_phrases.append(phrase)\n",
    "    return third_person_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_person_phrases=get_roots_third_person(texts_docs)\n",
    "print(len(third_person_phrases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Keeping only smaller sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large sentences will be harder to learn for the computer (as they are harder to understand for humans). Some guidelines indicate that an average of 15-20 words are clearer phrases. \n",
    "\n",
    "https://techcomm.nz/Story?Action=View&Story_id=106#:~:text=A%20common%20plain%20English%20guideline,2009%3B%20Vincent%2C%202014) -> (Cutts, 2009; Plain English Campaign, 2015; Plain Language Association InterNational, 2015)(Cutts, 2009; Vincent, 2014).\n",
    "\n",
    "Because of this, I excluded phrases that had more than 30 tokens (some tokens are punctuation so I left a bit of a buffer and this also allows for sentences that are a bit more complex) \n",
    "\n",
    "This step is a bit optional, but it removes almost 3000 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_phrases=[]\n",
    "for i in texts_docs:\n",
    "    if len(i)<30:\n",
    "        small_phrases.append(i)\n",
    "\n",
    "len(small_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_person_phrases_small=get_roots_third_person(small_phrases)\n",
    "print(len(third_person_phrases_small),'small 3rd person phrases')\n",
    "print(len(list(set(third_person_phrases_small))),'small different 3rd person phrases')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Deleting phrases that didn't end in '.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also deleted some phrases that didn’t end with a ‘.’ Because these phrases would not finish a thought (some phrases are not well obtained by the spacy model – more complex models give more accurate phrase delimitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_delete=[]\n",
    "n=0\n",
    "print('len inicial',len(third_person_phrases_small))\n",
    "for i in range(len(third_person_phrases_small)):\n",
    "    if third_person_phrases_small[i][-1].text!='.':\n",
    "        n+=1\n",
    "        indexes_to_delete.append(i)\n",
    "\n",
    "third_person_phrases_small=[third_person_phrases_small[i] for i in range(len(third_person_phrases_small)) if i not in indexes_to_delete]\n",
    "print(n)\n",
    "print('len final',len(third_person_phrases_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Remove sentences where the subject is 'it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neutral_small_phrases=[]\n",
    "\n",
    "\n",
    "for idx in range(len(third_person_phrases_small)):\n",
    "    for token in third_person_phrases_small[idx]:\n",
    "        if token.dep_=='ROOT':\n",
    "            root_child=[word for word in token.children]\n",
    "            for t in root_child:\n",
    "                if 'nsubj' in t.dep_ and t.morph.get('Gender')!=['Neut']:\n",
    "                     n_neutral_small_phrases.append(third_person_phrases_small[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(n_neutral_small_phrases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Selecting \"allowed\" sujects (nouns, pronouns and proper nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the ways to reduce the list of sentences more is selecting which kinds of things can be a subject such as proper nouns (even though these may be for example names of countries or cities), pronouns such as she/he/her/etc., and some common nouns that are related to humans such as \"the mother\".\n",
    "+ proper nouns: `token.pos_ == 'PROPN'`\n",
    "+ she/he: `token.morph.get('PronType')==['Prs] and (token.morph.get('Gender')==['Masc'] or token.morph.get('Gender')==['Fem']`\n",
    "+ common names: added to words to check (for example her father, etc - spacy identifies father as the subject in this case): `token.pos_=='NOUN`\n",
    "\n",
    "I also separated the phrases where the subject was a PROPN from the others because we may want to change these names from a predefined list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root_subjects(doc):\n",
    "    for token in doc:\n",
    "        if token.dep_=='ROOT':\n",
    "            root_child=[word for word in token.children]\n",
    "            for t in root_child:\n",
    "                if 'nsubj' in t.dep_:\n",
    "                    return t\n",
    "                    #returns a token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPN=[]\n",
    "pronouns=[]\n",
    "nouns=[]\n",
    "\n",
    "for idx in range(len(n_neutral_small_phrases)):\n",
    "        root_subject=get_root_subjects(n_neutral_small_phrases[idx])\n",
    "        if root_subject.pos_=='PROPN':\n",
    "            PROPN.append(n_neutral_small_phrases[idx])\n",
    "        elif root_subject.pos_=='NOUN':\n",
    "            nouns.append(n_neutral_small_phrases[idx])\n",
    "        elif root_subject.morph.get('PronType')==['Prs'] and (root_subject.morph.get('Gender')==['Masc'] or root_subject.morph.get('Gender')==['Fem']):\n",
    "            pronouns.append(n_neutral_small_phrases[idx])\n",
    "\n",
    "print('the list of phrases with proper nouns as subjects has {} elements'.format(len(PROPN)))\n",
    "print('the list of phrases with common nouns as subjects has {} elements'.format(len(nouns)))\n",
    "print('the list of phrases with he/she pronouns as subjects has {} elements'.format(len(pronouns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the \"raw\" three lists\n",
    "\n",
    "files = {\n",
    "    'PROPN_phrases.pkl': PROPN,\n",
    "    'pronouns_phrases.pkl': pronouns,\n",
    "    'nouns_phrases.pkl': nouns\n",
    "}\n",
    "\n",
    "for filename, list_ in files.items():\n",
    "    with open(os.path.join(DATA_PATH, filename), 'wb') as f:\n",
    "        pickle.dump(list_, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ======================= "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Choosing which of the subjects seem useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'PROPN_phrases.pkl'), 'rb') as f:\n",
    "    phrases_PROPN=pickle.load(f) \n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'pronouns_phrases.pkl'), 'rb') as f:\n",
    "    phrases_pron=pickle.load(f) \n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'nouns_phrases.pkl'), 'rb') as f:\n",
    "    phrases_nouns=pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these datasets I chose which would be the nouns and proper nouns that could be useful for the dataset so that I could restrict a bit more the phrases to choose by hand. To do that I printed the list of subjects of the dataframes and chose by hand the ones that seem to be related to persons/characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_nouns=[get_root_subjects(i) for i in phrases_nouns]\n",
    "print(subj_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_nouns_subject=['Victor','father','wife','girl','Fitzgerald','man','Torvald','Tashtego','Starbuck','woman',\n",
    "'mother','teacher','Farson', 'Browning','boy','sister','Utterson','Lanyon','boy','Jaggers','Estella','aunt','Jane','Rhys',\n",
    "'Hester','Heart','charwoman','Marlow','Svidrigailov','Cassedy','Huck','Sanders','prince','Murry','Swinburne',\n",
    "'Anatole','Deasy','Anatole','Chryses','Achilles','Diomedes','Hector','Thetis','Kleos','goddess','Agamemnon',\n",
    "'Scrooge','Danglars','Douglass','king','Fantine','Thénardier','Marius','grandfather','Eurycleia',\n",
    "'Penelope','Hobbes','Léonce','Reisz','Adèle','Chopin','Treatise','aunt','Vronsky','Levin','Eliza','Eva','husband',\n",
    "'Crisóstomo','Elías','Fagin','Cephalus','Pygmalion','Gyges','brother','Eliza','hero','heroine','Watson',\n",
    "'woman','gentleman','Sissy','uncle','overman','Wolper']\n",
    "\n",
    "#houskeeper? (it is neutral?) hunter leader carpenter narrator beggar bartender president precursor\n",
    "#assistant character friend minister accountant manager clerk author \n",
    "#child\n",
    "#Dracula is both the character and the book's name (see each phrase) - this may eventually also happen with Frankenstein\n",
    "#maid is always feminin? priest always male?\n",
    "#chairman/woman - maybe we can say \"nouns that countain man/woman\"  -> maybe this doesnt work because woman has the word man \n",
    "#Fate see if it is fate or a propnoun (appears with capslock - maybe im mistaking this with Faith); Hope\n",
    "#soldier at the time this was a male almost for sure but now it isn't, keep it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_after=[]\n",
    "for phrase in phrases_nouns:\n",
    "    if get_root_subjects(phrase).text in useful_nouns_subject:\n",
    "        nouns_after.append(phrase)\n",
    "\n",
    "print(len(nouns_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_PROPN=[get_root_subjects(i) for i in phrases_PROPN]\n",
    "print(subj_PROPN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_propn_subject=['Walton','Victor','Clerval','Mary','Blackwell','Toro','Bennet','Elizabeth','Collins','Darcy',\n",
    "    'Charlotte','Jane','Catherine','Fitzwilliam','Wickham','Lydia','Bingley','Catherine','Austen','Joyce','Reynolds',\n",
    "    'Alice','Tom','Jordan','Myrtle','Nick','Daisy','Fitzgerald','Gatsby','Wilde','George','Bechtel','Buchanan','Marx',\n",
    "    'Levy','Lorry','Evrémonde','Marquis','Gaspard','Carton','Darnay','Solomon','Defarge','Jerry','Manette',\n",
    "    'Lucie','Defarge','Darnay','Carlyle','Simon','Törnqvist','Nora','Kristine','Krogstad','Torvald','Rank',\n",
    "    'Nora','Kristine','Mencken','Stoddart','Dorial', 'Basil','Alan','James','Ishmael','Ahab','Queequeg','Stubb','Pip',\n",
    "    'Dick','Bryant','Bezanson','Wright','Arvin','Matthiessen','Melville','Bryant','Milder','Gilman','Lanser','Treichler',\n",
    "    'Horowitz','Edelstein','Harker','Lucy','Mina','Helsing','Showalter','Redmond','Enfield','Jekyll','Lanyon','Poole',\n",
    "    'Utterson','Hyde','Wright','Havisham','Joe','Biddy','Herbert','Molly','Drummle','Jaggers','Magwitch','Jane','Reed',\n",
    "    'Brocklehurst','Temple','Helen','Rochester','Rivers','John','Hester','Chillingworth','Pearl','Dimmesdale','Conrad',\n",
    "    'Hochschild','Marlow','Nylander','Samsa','Sudau','Rubio','Drüke','Nabokov','Frank','Snitkina','Raskolnikov',\n",
    "    'Marmeladov','Sonya','Razumikhin','Porfiry','Dunya','Mikolka','Luzhin','Lebezyatnikov','Svidrigailov','Dostoevsky',\n",
    "    'Huck','Jim','Loftus','Polly','Twain','Hearn','Alberti','Finn','Eltis','Ellmann','Aynesworth', 'Wilde','Jack',\n",
    "    'Bracknell','Gwendolen','Algernon','Gwendolen','Bracknell','Earnest','Foster','Edwards','Carby','Anderson',\n",
    "    'Sanders','Gilbert','Ulysses','Joyce','Stephen','Bloom','Mulligan','Gerty','Pierre',\n",
    "    'Boris','Rostov','Drubetskoy','Andrei','Denisov','Nikolai','Hélène','Pierre','Natasha','Rostov','Bolkonsky','Annenkov',\n",
    "    'Strakhov','Dunnigan','Bagnall','Moore','Walden','Agamemnon','Chryses','Odysseus','Thetis','Aphrodite',\n",
    "    'Athena','Diomedes','Zeus','Nestor','Poseidon','Polydamas','Hera','Achilles','Patroclus','Hephaestus','Hector','Priam',\n",
    "    'Homer','Arnold','Pan','Barrie','Peter','Wendy','Robertson','Maimie','Darling','Lily','Bell','Hook','Smee','Faria',\n",
    "    'Dantès','Mondego','Bertuccio','Carderousse','Andrea','Dorothy','Glinda','Henry','Oz','Beth','Jo','Meg','Laurence',\n",
    "    'Brooke','Laurie', 'Meg', 'Beth','Amy','Lizzie','Saxton','Alcott','Carol','Scrooge','Irving','Kelly','Jim',\n",
    "    'Potter','Joe','Becky','Douglas','Hugo','Myriel','Valjean','Fantine','Javert','Thénardier','Marius','Cosette',\n",
    "    'Gavroche','Enjolras','Perry','Karamazov','Pavlovich','Fyodorovich','Alyosha','Snegiryov','Smerdyakov','Grushenka',\n",
    "    'Katerina','Zosima','Dmitri','Ilyusha','Alyosha','Snegiryov','Ivan','Kolya','Quixote','Fernando','Sancho','Anne',\n",
    "    'Cervantes','Athena','Telemachus','Odysseus','Penelope','Hobbes','Doyle','Edna','Robert','Mary',\n",
    "    'Tully','Zuckert','Kenny','Emma','Knightley','Byrne','Henry','Karenin','Levin','Vronsky','Stiva','Bartlett',\n",
    "    'Heathcliff','Dean','Earnshaw','Hindley','Catherine','Edgar','Cathy','Lockwood','Wiltshire','Scott','Alejandro',\n",
    "    'Stewart','Alejandro','Eliza','Clare','Eva','Legree','Shelby','Dámaso','Crisóstomo','Guevarra','Salví','Tiago','Elías',\n",
    "    'Guevarra','María','Twist','Oliver','Brownlow','Fagin','Nancy','Sikes','Bumble','Brownlow','Nancy','Rose','Bumble',\n",
    "    'Bates','Buck','Mercedes','Thornton','Pizer','Gianquitto','Higgins','Eliza','Doolittle','Higgins',\n",
    "    'Polemarchus','Glaucon','Adeimantus','Gulliver','Mendez','Pedro','Jacobs','Brent','Martha','Jacobs',\n",
    "    'Benjamin','William','Benny','Ellen','Flint','Bruce','John','Faustus','Lucifer','Marianne','Brandon','Steele',\n",
    "    'Edward','Pollock','Favret','Holmes','Drebber','Crane','Rudkus','Jonas','Jurgis','Gradgrind','Bounderby','Stephen',\n",
    "    'Sparsit','Tom','Blackpool','Louisa','Gradgrind','Harthouse','Cecilia','Bitzer','Rachael','Bazalgette','Mills','Pooh',\n",
    "    'Hanyu','Charlie','Ashbee','Marcus','Mary','Weatherstaff','Fauntleroy','Gerzina','Masson','Burnett','Virgil','Dante',\n",
    "    'Beatrice','David','Peggotty','Dora','Agnes','Hollington','Leavis','Needham','Bottiglia','Dexter',\n",
    "    'Anne','Henrietta','Benwick','Clay','Russell','Croft','Isagani','Basilio','Simoun']\n",
    "\n",
    "maybe_useful_propn_subject=['Frankenstein','Mouse','Rabbit','Cat','Hatter','Queen','Turtle','Gardner','Duck',\n",
    "    'Dormouse','Turtle','Pequod','Dracula','Witch','Lion','Ghost']\n",
    "\n",
    "\n",
    "#some names are precedeed by miss & co - change this if the gender changes\n",
    "#in Alice in wonderland's story there are several characters that are named after animals, maybe these will be useful\n",
    "# because the phrases will work exchanging these names for human names - same for the wizard of oz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPN_after=[]\n",
    "PROPN_after_maybe=[]\n",
    "\n",
    "for phrase in phrases_PROPN:\n",
    "    if get_root_subjects(phrase).text in useful_propn_subject:\n",
    "        PROPN_after.append(phrase)\n",
    "    elif get_root_subjects(phrase).text in maybe_useful_propn_subject:\n",
    "        PROPN_after_maybe.append(phrase)\n",
    "\n",
    "print(len(PROPN_after))\n",
    "print(len(PROPN_after_maybe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Remove subjects that only appear once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also concluded that if a name only appears one time probably is not related to the plot of the story but it is probably just a comment of someone that studied the text which is not what we want to a removed those names from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_subj(list_of_subjects,list_docs):\n",
    "    #list_of_subjects is a list with strings that correspond to the root subjects of the spacy doc (snetence)\n",
    "    #list_docs is a list of Spacy docs (sentences)\n",
    "    single=[]\n",
    "    list_subj=[get_root_subjects(i) for i in list_docs]\n",
    "    for nn in list_of_subjects:\n",
    "        n=0\n",
    "        for i in range(len(list_subj)):\n",
    "            if list_subj[i].text==nn:\n",
    "                n+=1\n",
    "        if n==1:\n",
    "            single.append(nn)\n",
    "    return single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propn_after_single=[]\n",
    "single_subj_propn=single_subj(useful_propn_subject,PROPN_after)\n",
    "for phrase in PROPN_after:\n",
    "    if get_root_subjects(phrase).text not in single_subj_propn:\n",
    "        propn_after_single.append(phrase)\n",
    "\n",
    "print('before removing there were {} phrases with proper nouns as subjects and after removing there are {} '.format(len(PROPN_after), len(propn_after_single)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Deleting duplicated phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three dataframes are the ones that can be used to choose the phrases by hand:\n",
    "+ nouns_after - has 108 phrases (some of which are prop nouns that were wrongly selected as common nouns by spacy)\n",
    "+ propn_after_single - has 1149 phrases (some don't correspond to the plot but to researchers commenting on the meaning of the plots)\n",
    "+ phrases_pron - has 759 phrases and are all the phrases that have \"he/she\" as the subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_phrases(list_of_phrases):\n",
    "    same_phrases=[] #list of the duplicate phrase that appears in 2nd place\n",
    "\n",
    "    for phrase1_idx in range(len(list_of_phrases)):\n",
    "        for phrase2_idx in range(len(list_of_phrases)):\n",
    "            if list_of_phrases[phrase1_idx].text==list_of_phrases[phrase2_idx].text and phrase2_idx!=phrase1_idx:\n",
    "                same_phrases.append(list_of_phrases[phrase1_idx])\n",
    "\n",
    "    same_phrases=list(set(same_phrases))\n",
    "    \n",
    "    return same_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_propn=equal_phrases(propn_after_single)\n",
    "eq_nouns=equal_phrases(nouns_after)\n",
    "eq_pronouns=equal_phrases(phrases_pron)\n",
    "\n",
    "#takes a bit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(eq_propn))\n",
    "print(len(eq_nouns))\n",
    "print(len(eq_pronouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(propn_after_single))\n",
    "\n",
    "for i in eq_propn:\n",
    "    propn_after_single.remove(i)\n",
    "\n",
    "print(len(propn_after_single))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in eq_nouns:\n",
    "    nouns_after.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in eq_pronouns:\n",
    "    phrases_pron.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(propn_after_single))\n",
    "print(len(nouns_after))\n",
    "print(len(phrases_pron))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the deletion of the duplicates:\n",
    "+ nouns_after - has 197 phrases\n",
    "+ propn_after_single - has 1022 phrases\n",
    "+ phrases_pron - has 741 phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Highlighting of root verb and subject (function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the identification of the subjects easier the following function gives a version of a row with the subject and verb highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def highlight_word(phrase,colour_verb,colour_subj):\n",
    "    # phrase is a doc string\n",
    "    # colour is a string with rgb values (ex. 'rgb(155,217,230)')\n",
    "\n",
    "    sent=[]\n",
    "    root_verb=[root for root in phrase if root.dep_=='ROOT'][0]\n",
    "\n",
    "    for token in phrase:\n",
    "        if token.dep_=='ROOT':\n",
    "            sent.append(\" <span style='background: {}'>{}</span> \".format(colour_verb,token.text))\n",
    "        elif 'nsubj' in token.dep_ and token in root_verb.children:\n",
    "            sent.append(\" <span style='background: {}'>{}</span> \".format(colour_subj,token.text))\n",
    "        else:\n",
    "            sent.append(token.text)\n",
    "\n",
    "    \n",
    "    return ' '.join(sent) #returns a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nouns_after\n",
    "# propn_after_single \n",
    "# phrases_pron\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "colour_verb='rgb(25, 108, 56)'\n",
    "colour_subj='rgb(188, 108, 37)'\n",
    "\n",
    "p=phrases_pron[0]\n",
    "\n",
    "display(HTML(highlight_word(p,colour_verb,colour_subj)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Selection of useful phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can be used to iterate over a list of Spacy docs and presents the highlighted phrase. This is useful because it is easier to avaliate which are the useful phrases for the dataset. \n",
    "This selection is done by hand and the selected sentences should: \n",
    "+ be part of the plot and not someone unrelated to the plot speaking about it\n",
    "    + considerations about the author/book\n",
    "+ not contain citations ('this character said \"this\"')\n",
    "    + some contain \" \" \n",
    "    + some contain you/me \n",
    "+ phrases starting with -\"  Chapter x - ...\"\n",
    "+ errors\n",
    "    + in phrases (the Rachel, wrong words (81 from PROPN))\n",
    "    + phrases starting with lower case (don't have the entire thought)\n",
    "    + starting with numbers (\"(number)\")\n",
    "+ confusing phrases that usually have a '-' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the indexes to a list (because it is easier to write) but then added the sentences of these indexes to another list so that if we change the dataset, we don't lose the sentences already removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nouns_after\n",
    "# propn_after_single \n",
    "# phrases_pron\n",
    "\n",
    "for i in range(0,len(nouns_after)):\n",
    "    print(i)\n",
    "    display(HTML(highlight_word(nouns_after[i],colour_verb='rgb(25, 108, 56)',colour_subj='rgb(188, 108, 37)')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns_indexes_rem=[11,31,48,49,56,63,67,72,73,74,75,93,94,111,112,145,146,147,148,149,150,195,214,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,234,235,236,247,298,337,339,340,350,351,352,353,354,359,395,412,424,451,460,461,462,463,464,465,466,469,470,471,472,473,507,515,561,569,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,595,596,598,602,603,670,685,686,687,700,701,727,732,738,740]\n",
    "pronouns_del=[phrases_pron[i] for i in range(len(phrases_pron)) if i in pronouns_indexes_rem]\n",
    "pronouns_keep=[phrases_pron[i] for i in range(len(phrases_pron)) if i not in pronouns_indexes_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_indexes_rem=[17,18,35,36,47,50,51,68,69,70,75,87,88,91,97,102,111,112,132,142,143,175,190 ]\n",
    "nouns_del=[nouns_after[i] for i in range(len(nouns_after)) if i in nouns_indexes_rem]\n",
    "nouns_keep=[nouns_after[i] for i in range(len(nouns_after)) if i not in nouns_indexes_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPN_indexes_rem=[24,25,26,27,33,41,42,42,44,45,46,47,48,49,50,73,74,80,88,115,145,146,144,143,147,148,150,151,152,153,154,155,156,157,158,159,184,297,298,300,301,302,303,304,314,315,324,326,327,328,329,330,331,392,397,400,412,414,415,416,417,418,420,422,423,427,436,437,440,497,513,514,561,571,573,575,588,589,597,598,602,603,604,605,606,634,636,635,637,638,639,666,667,700,749,779,793,795,796,813,814,815,816,817,818,825,827,829,830,831,832833,839,840,850,851,864,920,921,922,923,927,928,933,934,935,936,937,946,947,948,949,950,965,966,967,968,969,970,971,972,973,974,993,994,995,996,1003,1019 ]\n",
    "PROPN_del=[propn_after_single[i] for i in range(len(propn_after_single)) if i in PROPN_indexes_rem]\n",
    "PROPN_keep=[propn_after_single[i] for i in range(len(propn_after_single)) if i not in PROPN_indexes_rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PROPN phrases to delete: {} \\n PROPN phrases to keep: {}'.format(len(PROPN_del),len(PROPN_keep)))\n",
    "print('nouns phrases to delete: {} \\n nouns phrases to keep: {}'.format(len(nouns_del),len(nouns_keep)))\n",
    "print('pronouns phrases to delete: {} \\n pronouns phrases to keep: {}'.format(len(pronouns_del),len(pronouns_keep)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this selection of the duplicates:\n",
    "+ nouns_keep - has 174 phrases\n",
    "+ PROPN_keep - has 876 phrases\n",
    "+ pronouns_keep - has 637 phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Saving the final phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "keep_files = {\n",
    "    'PROPN_keep.pkl': PROPN_keep,\n",
    "    'pronouns_keep.pkl': pronouns_keep,\n",
    "    'nouns_keep.pkl': nouns_keep\n",
    "}\n",
    "\n",
    "for filename, list_ in keep_files.items():\n",
    "    with open(os.path.join(DATA_PATH, filename), 'wb') as f:\n",
    "        pickle.dump(list_, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0568189e475ff095f8d58e708f11d5183d1019c3e0d991b7e7d0732f0bba373"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
